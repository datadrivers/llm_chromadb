{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95853677",
   "metadata": {},
   "source": [
    "# Goal of this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4df2ea35",
   "metadata": {},
   "source": [
    "We want to create a custom knowledge base about amazon sagemaker QNAs which we can chat to! Since the knowledge is very domain specific about AWS sagemaker, we could ask a LLM questions about Amazon or Amazon Web Services (AWS) in general, but we might struggle to find specific details about sagemaker. \n",
    "\n",
    "The general idea is: If we have sensitive data or company data, we can not use the OpenAI API, since we would send our documents to the API. This has two disadvantages:\n",
    "\n",
    "- we share our sensitive data with OpenAI\n",
    "- we need an API key for this, which means it costs us money per requests depending on the number of tokens we send to the API\n",
    "\n",
    "To be able to keep privacy of our data, we show a process how to use a local model, which will run on our computer, and use a local vector database for our domain specific knowledge. In this environment no data is leaving our local machine. This could, in a next step, be easily deploed in the cloud. For example on AWS sagemaker or GoogleCloud Vertext AI. The cost for this would be quite low. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fade883",
   "metadata": {},
   "source": [
    "# Setting up the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79c0bddf",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "\n",
    "- a notebook or computer with sufficient memory 16GB\n",
    "- python environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3d007f8",
   "metadata": {},
   "source": [
    "First we need to setup an environment. For the use of python, we recommend using pyenv virtualenv to setup a specific python version (in this case python 3.10.4) and use poetry to take care of the required python packages.\n",
    "\n",
    "To setup the environment, just use `pyenv virtualenv 3.10.4 llm_chatbot`\n",
    "\n",
    "Maybe an installation of python 3.10.4 is required before by using pyenv install 3.10.4. After the setup, just use pyenv activate llm_chatbot. When the python environment is activated, just use `poetry install`to install all required packages for this project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0d0a1c8",
   "metadata": {},
   "source": [
    "# Document Question Answering with local flant5-large\n",
    "\n",
    "An example of using Chroma DB and LangChain to do question answering over a sagemaker qna document with a local deployed llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "652985d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01fe5351",
   "metadata": {},
   "source": [
    "## Load documents\n",
    "\n",
    "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06619a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('Sagemaker_qna.txt')\n",
    "qna = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "478be861",
   "metadata": {},
   "source": [
    "## Split documents\n",
    "\n",
    "Now we want to split these documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "08eaaf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter_qna = RecursiveCharacterTextSplitter()\n",
    "texts_qna = text_splitter_qna.split_documents(qna)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b30aff",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB\n",
    "\n",
    "Create embeddings for each chunk and insert into the Chroma vector database. For this step, we first ne the correct emebdding to the model we want to use later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "989798d7",
   "metadata": {},
   "source": [
    "We want to use the [open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b) model with its embeddings for our vector db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6df214d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56206c50",
   "metadata": {},
   "source": [
    "Lets get the model from the huggingface_hub. Warning: this model has a size of almost 7GB, so please only download it on your laptop with a stable internet connection. Depending on your internet connection, this might take some minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3393192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c37d244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/jonasbechthold/.cache/torch/sentence_transformers/google_flan-t5-large. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/jonasbechthold/.cache/torch/sentence_transformers/google_flan-t5-large were not used when initializing T5EncoderModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5cd23c0",
   "metadata": {},
   "source": [
    "create the vector database with embeddings for our open_llama_3b model from huggingface to put our knowledge into the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0d2a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts_qna, embedding_function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64967063",
   "metadata": {},
   "source": [
    "## Local Model flant5large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5651a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ddb7866",
   "metadata": {},
   "source": [
    "## Create the chain\n",
    "\n",
    "Initialize the chain we will use for question answering. In this case we just use Langchain as a convenience tool, to tell the model which prompt with which input question it should use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89b54401",
   "metadata": {},
   "source": [
    "### create template and ask questions to flant5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b5f967",
   "metadata": {},
   "source": [
    "To tell the model how it should anser our question, we create a PromptTemplate from langchain to define a prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed80ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c2aace4",
   "metadata": {},
   "source": [
    "For this purpose we can use a langchain to connect the prompt with the llm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb5164cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=local_llm\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0615f7c1",
   "metadata": {},
   "source": [
    "Lets ask the llm a very general question, it should know the answer, since this is quite general which would be available from wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96a01b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Berlin. Berlin is located in Germany. So the final answer is Berlin.\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What is the capital of Germany?\"\n",
    "\n",
    "print(llm_chain.run(question1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e1a854f",
   "metadata": {},
   "source": [
    "# Create the chain with our vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1f859b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=local_llm, chain_type=\"stuff\", vectorstore=vectordb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cff96efe",
   "metadata": {},
   "source": [
    "## Ask questions!\n",
    "\n",
    "To have a comparison between the asked question without the doucment database, we ask a question to the pure llm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1072e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Amazon Sagemaker is a type of adverb. The Amazon Sagemaker is a type of adverb. The answer: adverb.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is amazon sagemaker?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b54fc125",
   "metadata": {},
   "source": [
    "The anser we get is quite strange. The llm seems not to have the appropriate knowledge. Now we ask the Vector DB with the llm on top of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5851c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is amazon sagemaker\"\n",
    "qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18cd929d",
   "metadata": {},
   "source": [
    "Now the answer is much more appropriate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b1055d2",
   "metadata": {},
   "source": [
    "Lets ask a few more questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28b0e8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jupyter'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What notebook instance types does amazon sagemaker provide? Whats the price for this?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dd4847a",
   "metadata": {},
   "source": [
    "this seems to be correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59cd4f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AWS pricing'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What do you know about the pricing of aws sagemaker?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb8f16f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon SageMaker pricing is based on the number of instances you need.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What do you know about the pricing of aws sagemaker? Give me some example prices!\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fafae286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Does Sagemaker support R?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "08a0f5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, R supported with Amazon SageMaker. You can use R within SageMaker Notebook instances, which include a pre-installed R kernel and the reticulate library.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Does sagemaker support R and in which applications?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06690b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With the new notebook experience, you can now quickly launch notebooks without needing to manually provision an instance and waiting for it to be operational'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How are SageMaker Studio Notebooks different from the notebooks instances?\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
